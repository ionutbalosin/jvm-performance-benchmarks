# Java Virtual Machine (JVM) Performance Benchmarks

This repository contains different JVM benchmarks for the C2/Graal JIT Compilers and the Garbage Collectors.

Each benchmark focuses on a specific execution pattern that is (potentially fully) optimized under ideal conditions (i.e., clean profiles). Such conditions might differ in real-life applications, so the benchmarks results are not always a good predictor on a larger scale. Even though the artificial benchmarks might not reveal the entire truth, they tell enough if properly implemented and executed.

For the full copyright and license information, please view the [LICENSE](LICENSE) file distributed with the source code.

## Authors

- [Ionut Balosin](https://www.ionutbalosin.com)
- [Florin Blanaru](https://twitter.com/gigiblender)

## Content

- [Purpose](#purpose)
- [JMH caveats](#jmh-caveats)
- [OS tuning](#os-tuning)
- [JVM coverage](#jvm-coverage)
- [JDK coverage](#jdk-coverage)
- [Benchmarks suites](#benchmarks-suites)
- [Infrastructure baseline benchmark](#infrastructure-baseline-benchmark)
- [Run the benchmarks](#run-the-benchmarks)
- [Benchmark plots](#benchmark-plots)

## Purpose

The main goal of the project is to assess:

1. different Compiler optimizations by following specific code patterns. At a first glance, even though some of these patterns might rarely appear directly in the user programs, they could occur after a few optimizations (e.g., inlining of high-level operations)
2. different Garbage Collectors' efficiency in both allocating but also reclaiming objects 

In addition, there is a small set of benchmarks (i.e., a macro category) covering larger programs (e.g., Fibonacci, Huffman coding/encoding, factorial, palindrome, etc.) using some high-level Java APIs (e.g., streams, lambdas, fork-join, etc.). Nevertheless, this is not the main purpose of this work.

We leave **out of scope** benchmarking any "syntactic sugar" language feature (e.g., records, sealed classes, pattern matching for the switch, local-variable type inference, etc.) as well as large applications (e.g., web-based microservices, etc.).

The benchmarks are written using [Java Microbenchmark Harness (JMH)](https://github.com/openjdk/jmh) which is an excellent tool for measuring the throughput and sampling latencies end to end.

## JMH caveats

### HotSpot-specific compiler hints

> JMH uses HotSpot-specific compiler hints to control the Just-in-Time (JIT) compiler. 

For that reason, the fully supported JVMs are all the HotSpot-based VMs, including vanilla OpenJDK and Oracle JDK builds. 
GraalVM is also supported.
For more details please check the [compiler hints](https://github.com/openjdk/jmh/blob/master/jmh-core/src/main/java/org/openjdk/jmh/runner/CompilerHints.java#L37) and [supported VMs](https://github.com/openjdk/jmh/blob/master/jmh-core/src/main/java/org/openjdk/jmh/runner/format/SupportedVMs.java#L31).

### Blackholes

> Using JMH Blackhole.consume() might dominate the costs, obscuring the results, in comparison to a normal Java-style source code.

Starting OpenJDK 17 the compiler supports blackholes [JDK-8259316](https://bugs.openjdk.org/browse/JDK-8259316). 
This optimization is available in [HotSpot](https://github.com/openjdk/jdk/blob/master/src/hotspot/share/opto/library_call.cpp#L7843) and the [Graal compiler](https://github.com/oracle/graal/blob/master/compiler/src/org.graalvm.compiler.nodes/src/org/graalvm/compiler/nodes/debug/BlackholeNode.java).

In order to perform a fair comparison between OpenJDK 11 and OpenJDK 17, compiler blackholes should be manually disabled in the benchmarks. 

The cost of `Blackhole.consume()` is zero (the compiler will not emit any instructions for the call) when compiler blackholes are enabled and supported by the top-tier JIT compiler of the underlying JVM. 

In case a benchmark annotated method returns a value instead of consuming it via `Blackhole.consume()` 
(e.g., the case of non-void benchmark methods), JMH will wrap the return value of the benchmark 
method in a blackhole, in order to avoid dead code elimination. 
In this case, blackholes are generated by the test infrastructure even though there is no explicit use of them in the benchmarks.  

> Explicitly using `Blackhole.consume()` (in hot loops) can result in misleading benchmark results, especially when compiler blackholes are disabled.

For that reason, to focus on broader benchmarks reusability (i.e., across different JDK versions and distributions) and increased test fidelity we recommend avoiding (whenever it is possible) the explicit usage of `Blackhole.consume()`.

## OS tuning

When doing benchmarking, it is recommended to disable potential sources of performance non-determinism. Below are described the tuning configurations the benchmark provides for each specific OS.

### Linux

The Linux tuning script [configure-linux-os.sh](./configure-linux-os.sh) triggers all the following:
- set CPU(s) isolation (with `isolcpus` or `cgroups`)
- disable address space layout randomization (ASLR)
- disable turbo boost mode
- set CPU governor to performance
- disable CPU hyper-threading

>Note: these configurations are tested on Ubuntu 22.04 LTS (i.e., a Debian based) Linux distribution.

For further references please check:
- [LLVM benchmarking tips](https://llvm.org/docs/Benchmarking.html#linux)
- [How to get consistent results when benchmarking on Linux?](https://easyperf.net/blog/2019/08/02/Perf-measurement-environment-on-Linux)

### macOS

For macOS, the Linux tunings described above are not applicable. For example, the Apple M1/M2 (ARM-based) chips do not have hyper-threading, nor a turbo-boost mode (i.e., these are specific to the Intel chips). In addition,  [disabling ASLR](https://opensource.apple.com/source/lldb/lldb-76/tools/darwin-debug/darwin-debug.cpp) looks more cumbersome, etc.

Due to these reasons, the script [configure-mac-os.sh](./configure-mac-os.sh) does not enable any specific macOS tuning configuration.

### Windows

Windows is not our main focus therefore the script [configure-win-os.sh](./configure-win-os.sh) does not enable any specific Windows tuning configuration.

## JVM coverage

The table below summarizes the JVM distributions included in the benchmark. For transparency reasons, we provide a short explanation of why the others are not supported.

No. | JVM distribution   | JDK versions |  Included
-------------- |--------------------|--------------| ----------
1 | OpenJDK HotSpot VM | 11, 17       | Yes
2 | GraalVM CE        | 11, 17       | Yes
3 | GraalVM EE        | 11, 17       | Yes
4 | Eclipse OpenJ9 VM  | N/A         | No, see the resons below
5 | Azul Prime (Zing)  | N/A         | No, see the resons below

### Eclipse OpenJ9 VM

JMH may functionally work with [Eclipse OpenJ9](https://www.eclipse.org/openj9). However, all the [compiler hints](https://github.com/openjdk/jmh/blob/master/jmh-core/src/main/java/org/openjdk/jmh/annotations/CompilerControl.java) will not apply to Eclipse OpenJ9 and this might lead to different results (i.e., unfair advantage or disadvantage, depending on the test).

For more details please check [JMH with OpenJ9](https://github.com/eclipse-openj9/openj9/issues/4649) and [Mark Stoodley on Twitter](https://twitter.com/mstoodle/status/1532344345524936704)

At the moment we leave it **out of scope** Eclipse OpenJ9 until we find a proper alternative.

### Azul Prime (Zing)

Publishing benchmark results for Azul Prime (Zing) is prohibited by [Azul's Evaluation Agreement](https://www.azul.com/wp-content/uploads/Azul-Platform-Prime-Evaluation-Agreement.pdf). 
It is only allowed to publish results for Azul Prime if prior consent is obtained from Azul.

As of now, we decided to skip this JVM in order to avoid the additional overhead of obtaining such consent.

## JDK coverage

At the moment the benchmark is configured to work only with the JDK Long-Term Support (LTS) versions.

No. | JVM distribution   | JDK versions |  Build
-------------- |--------------------|--------------| -------------------------------
1 | OpenJDK HotSpot VM | 11, 17       | [download](https://projects.eclipse.org/projects/adoptium.temurin/downloads/)
2 | GraalVM CE        | 11, 17       | [download](https://www.graalvm.org/downloads/)
3 | GraalVM EE        | 11, 17       | [download](https://www.graalvm.org/downloads/)

If there is a need for another JDK LTS version (or feature release), you have to configure it by yourself. 

Additionally, if you decide to install a different OpenJDK build, we recommend to take one with [Shenandoah GC](https://wiki.openjdk.org/display/shenandoah/Main) available.

### Configure JDK

After the JDK was installed, the JDK path needs to be updated in the benchmark configuration scripts.  To do so, open the [configure-jvm.sh](./configure-jvm.sh) script file and update the corresponding **JAVA_HOME** property:
```
export JAVA_HOME="<path_to_jdk>"
```

#### A few examples

Linux OS:
```
export JAVA_HOME="/usr/lib/jvm/openjdk-17.0.5"
```

Mac OS:
```
export JAVA_HOME="/Library/Java/JavaVirtualMachines/openjdk-17.0.5/Contents/Home"
```

Windows OS:
```
export JAVA_HOME="/c/Program_Dev/Java/openjdk-17.0.5"
```

## Benchmarks suites

The benchmarks are organized in suites (i.e., benchmarks suites). To run a benchmark suite on a JDK version it needs a very specific configuration. There are predefined benchmarks suites (in JSON configuration files) for each supported JDK LTS version:

- [benchmarks-suite-jdk11.json](./benchmarks-suite-jdk11.json)
- [benchmarks-suite-jdk17.json](./benchmarks-suite-jdk17.json)

The benchmark will sequentially pick up and execute all the tests from the configuration file.

There are a few reasons why such a custom configuration is needed:

- selectively pass different JVM arguments to subsequent runs of the same benchmark  (e.g., first run with ZGC, second run with G1GC, etc.) 
- selectively pass different JMH options to subsequent runs of the same benchmark (e.g., first run with one thread, second run with two threads, etc.)
- selectively control what benchmarks(, JVM parameters and JMH options) to include/exclude in one JDK version (e.g., exclude from JDK 11 the ZGC benchmark since it was experimental at that moment, etc.)

## Infrastructure baseline benchmark

We provide a baseline benchmark for the infrastructure, [InfrastructureBaselineBenchmark](./benchmarks/src/main/java/com/ionutbalosin/jvm/performance/benchmarks/InfrastructureBaselineBenchmark.java), that can be used to assess the infrastructure overhead for the code to measure.

It measures the performance of empty methods (w/ and w/o explicit inlining) but also the performance of returning an object versus consuming it via blackholes. All of these mechanisms are used inside the real suite of tests.

This benchmark is particularly useful in case of a comparison between different JVMs and JDKs, and it should be run before any other real benchmark to check the default costs.
In that regard, if the results of the infrastructure baseline benchmark are not the same, it does not make sense to compare the results of the other benchmarks between different
JVMs and JDKs.

## Run the benchmarks

Running the benchmarks triggers the full setup (in a very interactive way, so that the user can choose what steps to skip), prior to execute any benchmark, as follows:
- configure the OS
- configure the JVM (e.g., set JAVA_HOME, etc.)
- configure the JMH (e.g., choose the benchmark suite for the specific JDK, define the results output folder, etc.)
- compile the benchmarks source code (using a Maven profile for the specific JDK)

## Elapsed amount of time for each benchmark suite

Each benchmarks suite take a significant amount of time to fully run. For example:

 Benchmark suite |  Elapsed time
--------------| ----------
benchmarks-suite-jdk11.json       | ~ 65 hours
benchmarks-suite-jdk17.json       | ~ 80 hours

### Dry run

Dry run mode goes through and simulates all the commands, but without changing any OS setting, or executing any benchmark. We recommend this as a preliminary check before running the benchmarks.
```
./run-benchmarks.sh --dry-run
```

**Note:** Launch this command with `sudo` to simulate the OS configuration settings. This is needed, even in dry run mode, to read some system configuration files, otherwise not accessible. Nevertheless, it has no side effect on the actual OS settings.

### Normal run

```
./run-benchmarks.sh | tee run-benchmarks.out
```

**Note:** Launch this command with `sudo` to apply the OS configuration settings.

Each benchmark result is saved under `results/jdk-$JDK_VERSION/$ARCH/$JVM_NAME/$BENCHMARK_NAME.json`

### Bash scripts on Windows

To properly execute bash scripts on Windows there are a few alternatives:
- [GIT bash](https://git-scm.com/downloads)
- [Cygwin](https://www.cygwin.com/)
- Windows Subsystem for Linux (WSL)

## Benchmark plots

### Install R/ggplot2

The benchmark plot generation is based on [R/ggplot2](https://ggplot2.tidyverse.org/) that needs to be installed upfront.

### Generate the benchmark plots

To generate all benchmark plots corresponding to one `<jdk-version>` and (optionally,) a specific `<arch>`, run the below command:
```
./plot-benchmarks.sh <jdk-version> [<arch>]
```
If the `<arch>` parameter is omitted, it is automatically detected based on the current system architecture.

Each benchmark plot is saved under `results/jdk-$JDK_VERSION/$ARCH/$BENCHMARK_NAME.svg`.